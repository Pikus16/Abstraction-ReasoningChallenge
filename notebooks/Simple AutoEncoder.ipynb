{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple autoencoder example just to play around with how to ingest some of the data. This approach has several clearly obvious weaknesses, including the fact that it doesn't deal with the fact that different samples have different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from arc_code.data import get_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a really simple autoencoder (taken from https://towardsdatascience.com/implementing-an-autoencoder-in-tensorflow-2-0-5e86126e9f7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, intermediate_dim):\n",
    "        super(SimpleEncoder, self).__init__()\n",
    "        self.hidden_layer = tf.keras.layers.Dense(\n",
    "          units=intermediate_dim,\n",
    "          activation=tf.nn.relu,\n",
    "          kernel_initializer='he_uniform'\n",
    "        )\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "          units=intermediate_dim,\n",
    "          activation=tf.nn.sigmoid\n",
    "        )\n",
    "\n",
    "    def call(self, input_features):\n",
    "        activation = self.hidden_layer(input_features)\n",
    "        return self.output_layer(activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, intermediate_dim, original_dim):\n",
    "        super(SimpleDecoder, self).__init__()\n",
    "        self.hidden_layer = tf.keras.layers.Dense(\n",
    "          units=intermediate_dim,\n",
    "          activation=tf.nn.relu,\n",
    "          kernel_initializer='he_uniform'\n",
    "        )\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "          units=original_dim,\n",
    "          activation=tf.nn.sigmoid\n",
    "        )\n",
    "\n",
    "    def call(self, code):\n",
    "        activation = self.hidden_layer(code)\n",
    "        return self.output_layer(activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAutoencoder(tf.keras.Model):\n",
    "    def __init__(self, intermediate_dim, original_dim):\n",
    "        super(SimpleAutoencoder, self).__init__()\n",
    "        self.encoder = SimpleEncoder(intermediate_dim=intermediate_dim)\n",
    "        self.decoder = SimpleDecoder(intermediate_dim=intermediate_dim, original_dim=original_dim)\n",
    "  \n",
    "    def call(self, input_features):\n",
    "        code = self.encoder(input_features)\n",
    "        reconstructed = self.decoder(code)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, original):\n",
    "    reconstruction_error = tf.reduce_mean(tf.square(tf.subtract(model(original), original)))\n",
    "    return reconstruction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loss, model, opt, original):\n",
    "    with tf.GradientTape() as tape:\n",
    "        gradients = tape.gradient(loss(model, original), model.trainable_variables)\n",
    "        gradient_variables = zip(gradients, model.trainable_variables)\n",
    "        opt.apply_gradients(gradient_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = get_data('../data', training_set = True, grouped_by_problem = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_shapes = {}\n",
    "for entry in X_train:\n",
    "    if entry.shape not in unique_shapes:\n",
    "        unique_shapes[entry.shape] = [entry]\n",
    "    else:\n",
    "        unique_shapes[entry.shape].append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common shape is (10, 10) with 200 examples\n"
     ]
    }
   ],
   "source": [
    "# get the shape with the most elements\n",
    "max_len = -1\n",
    "max_len_shape = -1\n",
    "for shape,entries in unique_shapes.items():\n",
    "    if len(entries) > max_len:\n",
    "        max_len = len(entries)\n",
    "        max_len_shape = shape\n",
    "print(\"Most common shape is {} with {} examples\".format(max_len_shape, max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_samesize = np.zeros((max_len, max_len_shape[0], max_len_shape[1]), dtype = np.float32)\n",
    "Y_train_samesize = []\n",
    "counter = 0\n",
    "for i in range(len(X_train)):\n",
    "    if X_train[i].shape == max_len_shape:\n",
    "        X_train_samesize[counter] = X_train[i]\n",
    "        Y_train_samesize.append(Y_train[i])\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten training features\n",
    "#X_train_samesize = X_train_samesize.reshape(X_train_samesize.shape[0],\n",
    "#                                              X_train_samesize.shape[1] * X_train_samesize.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 125\n",
    "batch_size = 5\n",
    "autoencoder = SimpleAutoencoder(intermediate_dim=5, original_dim=10)\n",
    "opt = tf.optimizers.Adam(learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = tf.data.Dataset.from_tensor_slices(X_train_samesize)\n",
    "training_dataset = training_dataset.shuffle(X_train_samesize.shape[0])\n",
    "training_dataset = training_dataset.prefetch(batch_size * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 had average loss 4.212340354919434\n",
      "Epoch 1 had average loss 4.178206920623779\n",
      "Epoch 2 had average loss 4.142806529998779\n",
      "Epoch 3 had average loss 4.110197067260742\n",
      "Epoch 4 had average loss 4.0782012939453125\n",
      "Epoch 5 had average loss 4.047335147857666\n",
      "Epoch 6 had average loss 4.010562419891357\n",
      "Epoch 7 had average loss 3.9731171131134033\n",
      "Epoch 8 had average loss 3.9398117065429688\n",
      "Epoch 9 had average loss 3.9111602306365967\n",
      "Epoch 10 had average loss 3.8865966796875\n",
      "Epoch 11 had average loss 3.8653275966644287\n",
      "Epoch 12 had average loss 3.846630096435547\n",
      "Epoch 13 had average loss 3.8299922943115234\n",
      "Epoch 14 had average loss 3.8150482177734375\n",
      "Epoch 15 had average loss 3.801501512527466\n",
      "Epoch 16 had average loss 3.7891225814819336\n",
      "Epoch 17 had average loss 3.7777130603790283\n",
      "Epoch 18 had average loss 3.7671713829040527\n",
      "Epoch 19 had average loss 3.757396936416626\n",
      "Epoch 20 had average loss 3.7482993602752686\n",
      "Epoch 21 had average loss 3.739809036254883\n",
      "Epoch 22 had average loss 3.7318410873413086\n",
      "Epoch 23 had average loss 3.7243359088897705\n",
      "Epoch 24 had average loss 3.7172491550445557\n",
      "Epoch 25 had average loss 3.710554361343384\n",
      "Epoch 26 had average loss 3.704219102859497\n",
      "Epoch 27 had average loss 3.6982247829437256\n",
      "Epoch 28 had average loss 3.692542314529419\n",
      "Epoch 29 had average loss 3.6871540546417236\n",
      "Epoch 30 had average loss 3.682036876678467\n",
      "Epoch 31 had average loss 3.6771700382232666\n",
      "Epoch 32 had average loss 3.6725330352783203\n",
      "Epoch 33 had average loss 3.6681106090545654\n",
      "Epoch 34 had average loss 3.6638851165771484\n",
      "Epoch 35 had average loss 3.659846544265747\n",
      "Epoch 36 had average loss 3.6559760570526123\n",
      "Epoch 37 had average loss 3.652263641357422\n",
      "Epoch 38 had average loss 3.6487033367156982\n",
      "Epoch 39 had average loss 3.645284414291382\n",
      "Epoch 40 had average loss 3.6419904232025146\n",
      "Epoch 41 had average loss 3.6388137340545654\n",
      "Epoch 42 had average loss 3.635756492614746\n",
      "Epoch 43 had average loss 3.6328089237213135\n",
      "Epoch 44 had average loss 3.6299631595611572\n",
      "Epoch 45 had average loss 3.62722110748291\n",
      "Epoch 46 had average loss 3.6245720386505127\n",
      "Epoch 47 had average loss 3.622011423110962\n",
      "Epoch 48 had average loss 3.619532823562622\n",
      "Epoch 49 had average loss 3.617131233215332\n",
      "Epoch 50 had average loss 3.614795207977295\n",
      "Epoch 51 had average loss 3.612522602081299\n",
      "Epoch 52 had average loss 3.610320568084717\n",
      "Epoch 53 had average loss 3.608184337615967\n",
      "Epoch 54 had average loss 3.6061089038848877\n",
      "Epoch 55 had average loss 3.6040918827056885\n",
      "Epoch 56 had average loss 3.6021323204040527\n",
      "Epoch 57 had average loss 3.6002283096313477\n",
      "Epoch 58 had average loss 3.598374605178833\n",
      "Epoch 59 had average loss 3.596574544906616\n",
      "Epoch 60 had average loss 3.5948233604431152\n",
      "Epoch 61 had average loss 3.593116521835327\n",
      "Epoch 62 had average loss 3.591454267501831\n",
      "Epoch 63 had average loss 3.589831590652466\n",
      "Epoch 64 had average loss 3.588251829147339\n",
      "Epoch 65 had average loss 3.5867092609405518\n",
      "Epoch 66 had average loss 3.585204601287842\n",
      "Epoch 67 had average loss 3.5837321281433105\n",
      "Epoch 68 had average loss 3.582289934158325\n",
      "Epoch 69 had average loss 3.580887794494629\n",
      "Epoch 70 had average loss 3.5795135498046875\n",
      "Epoch 71 had average loss 3.5781710147857666\n",
      "Epoch 72 had average loss 3.5768613815307617\n",
      "Epoch 73 had average loss 3.5755839347839355\n",
      "Epoch 74 had average loss 3.574333906173706\n",
      "Epoch 75 had average loss 3.573111057281494\n",
      "Epoch 76 had average loss 3.5719127655029297\n",
      "Epoch 77 had average loss 3.5707454681396484\n",
      "Epoch 78 had average loss 3.5695993900299072\n",
      "Epoch 79 had average loss 3.568479061126709\n",
      "Epoch 80 had average loss 3.5673811435699463\n",
      "Epoch 81 had average loss 3.5663058757781982\n",
      "Epoch 82 had average loss 3.56524920463562\n",
      "Epoch 83 had average loss 3.5642168521881104\n",
      "Epoch 84 had average loss 3.5632033348083496\n",
      "Epoch 85 had average loss 3.5622076988220215\n",
      "Epoch 86 had average loss 3.56123685836792\n",
      "Epoch 87 had average loss 3.5602800846099854\n",
      "Epoch 88 had average loss 3.559339761734009\n",
      "Epoch 89 had average loss 3.558419704437256\n",
      "Epoch 90 had average loss 3.5575172901153564\n",
      "Epoch 91 had average loss 3.5566322803497314\n",
      "Epoch 92 had average loss 3.555758476257324\n",
      "Epoch 93 had average loss 3.5548973083496094\n",
      "Epoch 94 had average loss 3.5540480613708496\n",
      "Epoch 95 had average loss 3.5532186031341553\n",
      "Epoch 96 had average loss 3.5523996353149414\n",
      "Epoch 97 had average loss 3.5515940189361572\n",
      "Epoch 98 had average loss 3.550809383392334\n",
      "Epoch 99 had average loss 3.550030469894409\n",
      "Epoch 100 had average loss 3.549268960952759\n",
      "Epoch 101 had average loss 3.548516273498535\n",
      "Epoch 102 had average loss 3.547773838043213\n",
      "Epoch 103 had average loss 3.547043561935425\n",
      "Epoch 104 had average loss 3.546328067779541\n",
      "Epoch 105 had average loss 3.545619487762451\n",
      "Epoch 106 had average loss 3.5449233055114746\n",
      "Epoch 107 had average loss 3.5442399978637695\n",
      "Epoch 108 had average loss 3.5435657501220703\n",
      "Epoch 109 had average loss 3.5429041385650635\n",
      "Epoch 110 had average loss 3.542250871658325\n",
      "Epoch 111 had average loss 3.5416080951690674\n",
      "Epoch 112 had average loss 3.540975570678711\n",
      "Epoch 113 had average loss 3.540353536605835\n",
      "Epoch 114 had average loss 3.5397398471832275\n",
      "Epoch 115 had average loss 3.539135456085205\n",
      "Epoch 116 had average loss 3.5385429859161377\n",
      "Epoch 117 had average loss 3.5379555225372314\n",
      "Epoch 118 had average loss 3.5373778343200684\n",
      "Epoch 119 had average loss 3.5368101596832275\n",
      "Epoch 120 had average loss 3.5362493991851807\n",
      "Epoch 121 had average loss 3.5356950759887695\n",
      "Epoch 122 had average loss 3.5351505279541016\n",
      "Epoch 123 had average loss 3.534608840942383\n",
      "Epoch 124 had average loss 3.5340802669525146\n"
     ]
    }
   ],
   "source": [
    "all_loss = []\n",
    "for epoch in range(epochs):\n",
    "    for step, batch_features in enumerate(training_dataset):\n",
    "        train(loss, autoencoder, opt, batch_features)\n",
    "        loss_values = loss(autoencoder, batch_features)\n",
    "        all_loss.append(loss_values.numpy())\n",
    "        #original = tf.reshape(batch_features, (batch_features.shape[0], 10, 10, 1))\n",
    "        #reconstructed = tf.reshape(autoencoder(tf.constant(batch_features)), (batch_features.shape[0], 10, 10, 1))\n",
    "    ave_loss = np.average(all_loss)\n",
    "    print(\"Epoch {} had average loss {}\".format(epoch, ave_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
